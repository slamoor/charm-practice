
Running as 2 OS processes: hello --mca opal_warn_on_missing_libcuda 0
charmrun> /usr/bin/setarch x86_64 -R mpirun -np 2 hello --mca opal_warn_on_missing_libcuda 0
Charm++> Running on MPI library: Open MPI v4.1.1, package: Open MPI devspackswinst@dev-1.zaratan.umd.edu Distribution, ident: 4.1.1, repo rev: v4.1.1, Apr 24, 2021 (MPI standard: 3.1)
Charm++> Level of thread support used: MPI_THREAD_FUNNELED (desired: MPI_THREAD_FUNNELED)
Charm++> Running in SMP mode: 2 processes, 1 worker threads (PEs) + 1 comm threads per process, 2 PEs total
Charm++> The comm. thread both sends and receives messages
Converse/Charm++ Commit ID: v7.1.0-devel-193-gdb72e7370
Isomalloc> Synchronized global address space.
CharmLB> Load balancer assumes all CPUs are same.
Charm++> Running on 2 hosts
Charm++> cpu topology info is gathered in 0.059 seconds.
--------------------------------------------------------------------------
ORTE has lost communication with a remote daemon.

  HNP daemon   : [[20593,0],0] on node compute-a7-1
  Remote daemon: [[20593,0],1] on node compute-a7-3

This is usually due to either a failure of the TCP network
connection to the node, or possibly an internal failure of
the daemon itself. We cannot recover from this failure, and
therefore will terminate the job.
--------------------------------------------------------------------------
slurmstepd: error: *** JOB 1177570 ON compute-a7-1 CANCELLED AT 2023-02-15T23:46:57 DUE TO TIME LIMIT ***
